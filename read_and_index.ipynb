{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zwarte Piet tweets dataset\n",
    "Read and index with ElasticSearch the local json dataset. Various trimming and cleaning is performed to minimize the index size. After indexing, we perform some simple queries using the elasticsearch-dsl library. The script assumes that the local json datasets are stored in a folder named \"Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import os\n",
    "\n",
    "es = Elasticsearch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define which fields from the original dataset json files would be included in the ES database. The original set contains more than 1000 fields per item/tweet; most of which are not interesting to us. Each tweet comes with a user field as well, containing the info of the user the posted the tweet. The user field shoule also be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_keys = [\"created_at\",\"id\",\"text\",\"user\",\"retweet_count\",\n",
    "                \"favorite_count\",\"lang\",\"activist\",\"zp\"]\n",
    "include_user_keys  = [\"id\",\"location\",\"description\",\"followers_count\",\n",
    "                      \"friends_count\",\"listed_count\",\"favourites_count\",\"verified\",\"lang\",\"statuses_count\",\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed with indexing the \"filtered\" dataset. First, we read the local json files.  This might take time to complete especially if all datasets are used. As such, for demo purposes we set NUM (num of json files) set to 1 and TWEETSPERDATA is set to 100. For the final version, all the data should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder Data/with_status_count/activist/zp\n",
      "zp_filter_wstatusactivist_piet90.json\n",
      "zp_filter_wstatusactivist_piet80.json\n",
      "zp_filter_wstatusactivist_piet40.json\n",
      "Reading folder Data/with_status_count/activist/non_zp\n",
      "zp_filter_wstatusactivist_no_piet30.json\n",
      "zp_filter_wstatusactivist_no_piet10.json\n",
      "zp_filter_wstatusactivist_no_piet110.json\n",
      "Reading folder Data/with_status_count/verified/zp\n",
      "zp_filter_wstatusverified_piet40.json\n",
      "zp_filter_wstatusverified_piet60.json\n",
      "zp_filter_wstatusverified_piet20.json\n",
      "Reading folder Data/with_status_count/verified/non_zp\n",
      "zp_filter_wstatusverified_no_piet10.json\n",
      "zp_filter_wstatusverified_no_piet30.json\n",
      "zp_filter_wstatusverified_no_piet70.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Filtered \"\"\"\n",
    "TWEETSPERDATA = 500\n",
    "NUM = 3\n",
    "\n",
    "def readJsonFolder(folder):\n",
    "    \"\"\" \n",
    "        Function to read jsons from a folder. Each json \n",
    "        contains multiple items. All items are concatenated\n",
    "        into a single output json file.\n",
    "    \"\"\"\n",
    "    print('Reading folder %s'%folder)\n",
    "    LOC = folder\n",
    "    data = []\n",
    "    for file in os.listdir(LOC)[:NUM]:\n",
    "        print(file)\n",
    "        with open(LOC+\"/\"+file) as json_file: \n",
    "            try:\n",
    "                data += json.load(json_file)\n",
    "            except:\n",
    "                print(\"%s skipped\"%file)\n",
    "    return data\n",
    "\n",
    "act_zp = readJsonFolder('Data/with_status_count/activist/zp')\n",
    "act_nzp = readJsonFolder('Data/with_status_count/activist/non_zp')\n",
    "ver_zp = readJsonFolder('Data/with_status_count/verified/zp')\n",
    "ver_nzp = readJsonFolder('Data/with_status_count/verified/non_zp')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets need to be enriched with expert sentiment annotations. We read the annotations from the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All verified accounts</th>\n",
       "      <th>Catarina</th>\n",
       "      <th>Thirza</th>\n",
       "      <th>Combined</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SaskiaBelleman</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umarebru</td>\n",
       "      <td>Pro</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>Pro</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OmropFryslan</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlexanderNL</td>\n",
       "      <td>Con</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>Con</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YarnoRitzen</td>\n",
       "      <td>Con</td>\n",
       "      <td>con</td>\n",
       "      <td>Con</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  All verified accounts Catarina      Thirza Combined  Unnamed: 4\n",
       "0        SaskiaBelleman  Neutral     neutral  Neutral         NaN\n",
       "1              umarebru      Pro  irrelevant      Pro         NaN\n",
       "2          OmropFryslan  Neutral  irrelevant  Neutral         NaN\n",
       "3           AlexanderNL      Con  irrelevant      Con         NaN\n",
       "4           YarnoRitzen      Con         con      Con         NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "annotations_act = pd.read_csv(\"Data/zpiet-activist-sentiment.csv\", sep = \";\",  header='infer')\n",
    "annotations_act.head()\n",
    "\n",
    "annotations_ver = pd.read_csv(\"Data/zpiet-sentiment-verified.csv\", sep = \";\",  header='infer')\n",
    "annotations_ver.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing part in ES is tricky. Throwing everything in ES without taking care of certain fields' particular properties will not allow us to answer certain research questions (e.g. wordclouds). So here's what we do:\n",
    "- The **\"text\"** field is the most important. When queried, we want it to be tokenized and treated as a collection of words (instead of the typical __keyword__ type). We also want to remove any Dutch stopwords. To achieve these, we define our own **analyzer** when creating the index. And we assign the **text** to be of text type. Its \"fielddata\" is set to true so that it's indexable and aggregable.\n",
    "- We incorporate the folder names e.g. activist, non_zp as additional fields in the json files. If a json file comes from the activist folder then its activist field is set to true. If it comes from the verified folder, the its activist field is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n",
      "Activist: 1, ZP: 1, size: 3111\n",
      "Activist: 1, ZP: 0, size: 30392\n",
      "Activist: 0, ZP: 1, size: 391\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Activist: 0, ZP: 0, size: 33218\n",
      "nan\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Deletion of the index if it already exists\n",
    "es.indices.delete(index=\"zpiet-with_status_count-tweets-index\", ignore=[400, 404])\n",
    "\n",
    "# Create index with settings; for more check https://github.com/elastic/elasticsearch-py/blob/master/example/load.py\n",
    "index_body = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_dutch_analyzer\": {\n",
    "          \"type\": \"standard\",         \n",
    "          \"stopwords\": \"_dutch_\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "      \"index.mapping.total_fields.limit\":2000\n",
    "  },\n",
    " \"mappings\":{\n",
    "    \"tweet\":{\n",
    "        \"properties\": {\n",
    "          \"text\": {\"type\": \"text\",\"analyzer\": \"my_dutch_analyzer\",\"fielddata\":\"true\"} # fielddata is needed for term search, wordclouds\n",
    "         }  \n",
    "    }\n",
    "}\n",
    "}\n",
    "\n",
    "print(\"Indexing...\")\n",
    "es.indices.create(index=\"zpiet-with_status_count-tweets-index\",body=index_body)\n",
    "i=0\n",
    "for dataset,act,zp in [(act_zp,True,True), (act_nzp,True,False), (ver_zp,False,True),(ver_nzp, False,False)]:\n",
    "    print(\"Activist: %d, ZP: %d, size: %d\"%(act,zp,len(dataset)))\n",
    "    for doc in dataset[:min(len(dataset),TWEETSPERDATA)]:\n",
    "        doc.update({\"activist\":act})\n",
    "        doc.update({\"zp\":zp})\n",
    "        \n",
    "        # Delete unwanted keys\n",
    "        to_delete = []\n",
    "        for key in doc.keys():\n",
    "            if key not in include_keys: to_delete.append(key)\n",
    "        for key in to_delete: doc.pop(key, None)\n",
    "        \n",
    "        # Enrich with sentiment for the user\n",
    "        name = doc[\"user\"][\"screen_name\"]\n",
    "        #print(doc[\"user\"])\n",
    "        #break\n",
    "        combined_sentiment = \"unknown\"\n",
    "        sent = annotations_act.loc[annotations_act['random sample of non-verified accounts'] == name][\"Combined\"].values\n",
    "        if len(sent)>0: combined_sentiment = sent[0]\n",
    "        sent = annotations_ver.loc[annotations_ver['All verified accounts'] == name][\"Combined\"].values\n",
    "        if len(sent)>0: combined_sentiment = sent[0]\n",
    "        \n",
    "        doc[\"user\"].update({\"sentiment\":combined_sentiment})\n",
    "            \n",
    "        # Delete unwanted keys in user key\n",
    "        to_delete = []\n",
    "        for key in doc[\"user\"].keys():\n",
    "            if key not in include_user_keys: to_delete.append(key)\n",
    "        for key in to_delete: doc[\"user\"].pop(key, None)\n",
    "        \n",
    "        # Index\n",
    "        try:\n",
    "            res = es.index(index=\"zpiet-with_status_count-tweets-index\", doc_type='tweet', id=i, body=doc)\n",
    "        except:\n",
    "            print(\"Document not indexed probably due to sentiment value: \",combined_sentiment)\n",
    "        i+=1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticSearch quering\n",
    "Some simple elastic search quering for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query combination results:\n",
      "3.8175476 Anti-zwarte piet. Niemand is tegen een piet zonder de racistische blackface schmink https://t.co/kWIL73tQ4Q Amsterdam, NL\n",
      "3.594859 RT @rzuaslan: Niet anti-piet demonstrant @Politie_Rdam, maar anti-zwarte piet demonstrant. https://t.co/OKmNF3AtLR Rotterdam\n",
      "3.5371408 RT @Co0ontje: Piet! Piet! https://t.co/fJAWVnhzPY Nederlands Neanderthalië\n",
      "3.4777672 @OekelSjef @Burgemeester17 Sinterklaas op een roetveeg piet! Racisme van zebra's! Nederland\n",
      "3.4526117 @Ruijgrok020 @rickdus Als een Zwarte Piet die wel werkt! Nederland\n",
      "3.398516 Foto’s! Zwarte Piet is nog altijd ZWART in Leeuwarden https://t.co/6J7DPS90nd Nederland\n",
      "3.398516 RT @nol_123: zo zijn er nog anti piet figuren over de vangrail geknikkerd #dTV \n",
      "3.398516 RT @EWdeVlieger: Zwarte Piet is cynisme https://t.co/yZxpZkGcHi \n",
      "3.371166 @annefleurdd Je bedoelt ‘JIJ VINDT dat zwarte piet racisme is. \n",
      "3.3660238 @jndkgrf Het ultieme bewijs dat de Anti-Piet zooi kan oprotten !!\n",
      "en een beetje snel graag.\n",
      "OPROTTEN ANTI-PIET ZOOTJE \n",
      "3.3510828 RT @trouw: Gesloopte Piet, wie ziet hem niet? Een nieuw middel in de strijd van activisten tegen Zwarte Piet is opgedoken: het fijnknijpen… Weert, Nederland\n",
      "3.3510828 RT @trouw: Gesloopte Piet, wie ziet hem niet? Een nieuw middel in de strijd van activisten tegen Zwarte Piet is opgedoken: het fijnknijpen… \n",
      "3.2960072 @klup Dat er racisten bestaan is duidelijk. Dat maakt zwarte Piet nog niet racistisch. \n",
      "3.275207 @HenryJP5 @rose_m1 Neuken met een \"zwarte piet\" is het doen met dieren West friesland \n",
      "3.2572553 Heel wit nederland komt op voor een zwarte piet,hoe kan dat racisme zijn. https://t.co/NznucBZ4Se Noord-Brabant, Nederland\n",
      "3.2446685 RT @SybrenKooistra: Nazi's met pro zwarte piet spandoeken, staand naast kinderen.\n",
      "Hooligans die zwarte piet liederen zingen terwijl ze inti… Bronckhorst, Nederland\n",
      "3.1998394 @annefleurdd Correctie :Zwarte Piet racistisch noemen is racisme. Nederland\n",
      "3.194039 Zwarte Piet is aan het veranderen omdat een kleine minderheid dat wil. \n",
      "\n",
      "Over onderdrukking en discriminatie gespro… https://t.co/QjcIADmlMk Annerveenschekanaal, Drenthe\n",
      "3.194039 RT @hulswood: Tegenstander Zwarte Piet opgepakt na bedreigende \"kinderen onder botsplinters\"-uitspraken.\n",
      "....had al gebiedsverbod, maar is… West friesland \n",
      "3.192701 RT @mediatvnl: Het blijkt te gaan om het bekende spandoek met de tekst “Zwarte Piet is racisme”. Voorstanders van Zwarte Piet hebben geprob… Medemblik, Nederland\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query combination \"\"\"\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import MultiMatch, Match\n",
    "from elasticsearch_dsl import Q\n",
    "\n",
    "def exportJson(j, outfile):\n",
    "    with open(outfile, \"w\") as data_file:\n",
    "        json.dump(j, data_file, indent=2)\n",
    "\n",
    "m1 = Q(\"match\", text='piet') & Q(\"match\", activist=\"true\") & Q(\"match\", zp=\"true\")\n",
    "\n",
    "s = Search().using(es).query(m1)[0:20]   # Filter out stuff e.g. negative sentiment .exclude(\"match\", sentiment=\"negative\")\n",
    "response = s.execute()\n",
    "exportJson(response.to_dict(),\"res.json\")\n",
    "print()\n",
    "print('Query combination results:')\n",
    "for hit in response: print(hit.meta.score, hit.text, hit.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Got 279 Hits:\n",
      "Got 10 Buckets:\n",
      "{'key': 'piet', 'doc_count': 279, 'score': 3.301075268817204, 'bg_count': 279}\n",
      "{'key': 'zwarte', 'doc_count': 248, 'score': 2.5464304884594733, 'bg_count': 276}\n",
      "{'key': 'kick', 'doc_count': 79, 'score': 0.9046422741133754, 'bg_count': 81}\n",
      "{'key': 'out', 'doc_count': 79, 'score': 0.8487459731262144, 'bg_count': 85}\n",
      "{'key': 'anti', 'doc_count': 37, 'score': 0.2980888042579733, 'bg_count': 49}\n",
      "{'key': 'rt', 'doc_count': 171, 'score': 0.2376259006930674, 'bg_count': 530}\n",
      "{'key': 'bus', 'doc_count': 16, 'score': 0.1747997615047869, 'bg_count': 17}\n",
      "{'key': 'politie', 'doc_count': 28, 'score': 0.15143690343135366, 'bg_count': 48}\n",
      "{'key': 'demonstranten', 'doc_count': 15, 'score': 0.1502723878313032, 'bg_count': 17}\n",
      "{'key': 'demonstreren', 'doc_count': 13, 'score': 0.13949864834360154, 'bg_count': 14}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This is the typical ES quering. It would be easier with es-dsl.\"\"\"\n",
    "print()\n",
    "body = {\n",
    "    \"query\" : {\n",
    "        \"match\" : {\"text\" : \"piet\"}\n",
    "    },\n",
    "    \"aggregations\" : {\n",
    "        \"my_sample\" : {\n",
    "            \"sampler\" : {\n",
    "                \"shard_size\" : 100000\n",
    "            },\n",
    "            \"aggregations\": {\n",
    "                \"keywords\" : {\n",
    "                    \"significant_text\" : { \"field\" : \"text\" }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"zpiet-with_status_count-tweets-index\", body = body, size=80)\n",
    "with open(\"res.json\", \"w\") as data_file:\n",
    "    json.dump(res, data_file, indent=2)\n",
    "\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "print(\"Got %d Buckets:\" % len(res['aggregations']['my_sample'][\"keywords\"][\"buckets\"]))\n",
    "for i,hit in enumerate(res['aggregations']['my_sample'][\"keywords\"][\"buckets\"]):\n",
    "    #print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'bool': {'must': [{'multi_match': {'fields': ['text'], 'query': 'zwarte piet', 'type': 'phrase'}}, {'match': {'activist': 'false'}}, {'match': {'zp': 'true'}}]}}, 'aggs': {'my_sample': {'significant_text': {'field': 'text'}}}, 'from': 0, 'size': 3000}\n",
      "Got 116 Hits:\n",
      "Got 10 Buckets:\n",
      "{'key': 'zwarte', 'doc_count': 116, 'score': 28.325757575757...}\n",
      "{'key': 'piet', 'doc_count': 116, 'score': 27.32439024390244...}\n",
      "{'key': 'demonstreren', 'doc_count': 8, 'score': 5.454458977...}\n",
      "{'key': 'kick', 'doc_count': 18, 'score': 5.437294887039239,...}\n",
      "{'key': 'out', 'doc_count': 18, 'score': 5.023038049940548, ...}\n",
      "{'key': 't.co', 'doc_count': 47, 'score': 4.747380250345469,...}\n",
      "{'key': 'https', 'doc_count': 47, 'score': 4.572489428847294...}\n",
      "{'key': 'tijdens', 'doc_count': 5, 'score': 4.27207193816884...}\n",
      "{'key': 'anti', 'doc_count': 14, 'score': 4.0050390069893576...}\n",
      "{'key': '18', 'doc_count': 5, 'score': 3.5528760404280626, '...}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query combination and Aggregations with DSL\"\"\"\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import MultiMatch, Match\n",
    "from elasticsearch_dsl import Q\n",
    "from elasticsearch_dsl import A\n",
    "\n",
    "def exportJson(j, outfile):\n",
    "    with open(outfile, \"w\") as data_file:\n",
    "        json.dump(j, data_file, indent=2)\n",
    "\n",
    "m1 = Q(\"multi_match\",fields = ['text'] , query='zwarte piet', type=\"phrase\" ) & Q(\"match\", activist=\"false\") & Q(\"match\", zp=\"true\")\n",
    "a = A('significant_text', field='text')\n",
    "\n",
    "s = Search() #.aggs.bucket(\"keywords\", a).using(es).query(m1)[0:20]\n",
    "s = s.using(es).query(m1)[0:3000] \n",
    "s.aggs.bucket(\"my_sample\", a)\n",
    "print(s.to_dict())\n",
    "  # Filter out stuff e.g. negative sentiment .exclude(\"match\", sentiment=\"negative\")\n",
    "res = s.execute()\n",
    "exportJson(res.to_dict(),\"res.json\")\n",
    "\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "print(\"Got %d Buckets:\" % len(res['aggregations']['my_sample'][\"buckets\"]))\n",
    "for i,hit in enumerate(res['aggregations']['my_sample'][\"buckets\"]):\n",
    "    #print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])\n",
    "    print(hit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
